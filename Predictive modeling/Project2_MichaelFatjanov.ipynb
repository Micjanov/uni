{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Project 2 Predictive Modeling\n",
    "### Michael Fatjanov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project, we will build a SVC model with a linear kernel and train it with mostly word embeddings. Other features and models are also used and explained later.\n",
    "## Modules\n",
    "First, the necessary modules are imported. *Gensim* is used to import word embeddings. From *sklearn* we will use different models such as Support Vector Classifiers (SVC), Multi-layer Perceptron Classifier (MLPC) and XGBoost. Lastly, *nltk* is used to lemmatize the words and to obtain part-of-speech tags of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of training dataset\n",
    "To train the model, we will use [this Kaggle dataset](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus). This contains 1048575 words extracted by Groningen Meaning Bank and is also annotated for Named Entity Recognition usecases so we don't have to manually label the data by ourselves. It provides two kinds of features: Part of Speech (POS) tags and the entity. In this project, we will specifically look for location so we can assign label 1 for words which have tag 'I-geo' or 'B-geo' and label 0 for all other words. These are the labels to be predicted later.\n",
    "\n",
    "Lemmatization is also performed (converting every word to its base root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('archive/ner_dataset.csv', encoding='latin1')\n",
    "\n",
    "label = []\n",
    "for i in df['Tag']:\n",
    "    if i == 'I-geo' or i == 'B-geo':\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "df['label'] = label\n",
    "\n",
    "df['lemma'] = [lem.lemmatize(i) for i in df['Word']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also take the part-of-speech tags into account, but first we have to assign a numerical label to those so it becomes understandable for a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "POS_set = set(df['POS'].tolist())\n",
    "POS_list = (list(POS_set))\n",
    "\n",
    "le.fit(POS_list)\n",
    "POS = le.transform(df['POS'].tolist())\n",
    "df['labelPOS'] = POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sentence #           Word  POS    Tag  label         lemma\n",
      "0   Sentence: 1      Thousands  NNS      O      0     Thousands\n",
      "1           NaN             of   IN      O      0            of\n",
      "2           NaN  demonstrators  NNS      O      0  demonstrator\n",
      "3           NaN           have  VBP      O      0          have\n",
      "4           NaN        marched  VBN      O      0       marched\n",
      "5           NaN        through   IN      O      0       through\n",
      "6           NaN         London  NNP  B-geo      1        London\n",
      "7           NaN             to   TO      O      0            to\n",
      "8           NaN        protest   VB      O      0       protest\n",
      "9           NaN            the   DT      O      0           the\n",
      "10          NaN            war   NN      O      0           war\n",
      "11          NaN             in   IN      O      0            in\n",
      "12          NaN           Iraq  NNP  B-geo      1          Iraq\n",
      "13          NaN            and   CC      O      0           and\n",
      "14          NaN         demand   VB      O      0        demand\n",
      "15          NaN            the   DT      O      0           the\n",
      "16          NaN     withdrawal   NN      O      0    withdrawal\n",
      "17          NaN             of   IN      O      0            of\n",
      "18          NaN        British   JJ  B-gpe      0       British\n",
      "19          NaN         troops  NNS      O      0         troop\n",
      "20          NaN           from   IN      O      0          from\n",
      "21          NaN           that   DT      O      0          that\n",
      "22          NaN        country   NN      O      0       country\n",
      "23          NaN              .    .      O      0             .\n",
      "24  Sentence: 2       Families  NNS      O      0      Families\n",
      "25          NaN             of   IN      O      0            of\n",
      "26          NaN       soldiers  NNS      O      0       soldier\n",
      "27          NaN         killed  VBN      O      0        killed\n",
      "28          NaN             in   IN      O      0            in\n",
      "29          NaN            the   DT      O      0           the\n"
     ]
    }
   ],
   "source": [
    "print(df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048575\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the feature set\n",
    "Here, *GloV*e pre-trained word embeddings will be imported *via* the *Gensim* library. This specific vocabulary contains vectors of 6 billion tokens extracted from *Wikipedia* and *Gigaword* and the vectors are 50 units long. I have tried to work with embeddings of 300 units long, but building feature sets was in this case not feasible on a laptop with 16 GB of RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove2word2vec('glove.6B.50d.txt', 'glove')\n",
    "w2v = KeyedVectors.load_word2vec_format('glove', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a list of word embeddings was made for every word in the training dataset. *Gensim* will throw an error when a word doesn't exist in the loaded vocabulary, so those words are assigned a padding vector containing 50 zeros. The length of this list is compaired to the amount of words to ensure the code has not missed a word.\n",
    "\n",
    "Variables that might take a while to build, are pickled (dumping Python variables onto local files that can be loaded later) to avoid waiting all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048575\n",
      "1048575\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "vectorlist = []\n",
    "l = 50 * [0]\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        vectorlist.append(w2v[df['lemma'][i]].tolist())\n",
    "    except KeyError:\n",
    "        vectorlist.append(l)\n",
    "print(len(vectorlist))\n",
    "\n",
    "outfile = open('vectorlist','wb')\n",
    "pickle.dump(vectorlist, outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open('df','wb')\n",
    "pickle.dump(df, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('vectorlist','rb')\n",
    "vc2 = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open('df','rb')\n",
    "df2 = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all features consired to train the model later. For a word in dataset, a dictionary is made consisting of all of the word's features. The features considered here are the vectors of the main word and the 3 words before and the 3 words after it. For the main word, there is a label included telling if it is made out of digits since these have a very small chance of being a location. There are also labels containing info on the capitalization of all 7 words since practically all locations would be capitalized.\n",
    "\n",
    "For words in the beginning or the end of the dataset, padding vectors and padding labels are added as a vector of 50 zeros and zeros respectively. For example, the very first word of the dataset doesn't have words before it, so padding is added there. In hindsight, approaching the dataset as a circular unit might have been a better idea in this case.\n",
    "\n",
    "Unfortunately due to time constraints, I could not put as many features as I wanted to. The feature set takes at least 3 hours to build so it wasn't practical to easily add and remove features. That's also why I did not put everything in one dataframe since it would likely reach the limits of my system. In hindsight, I could have also test out features on a smaller dataset. \n",
    "\n",
    "I added and trained a model that is trained with POS tags later on in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(i, df, vc):\n",
    "    lemma = df['lemma'].tolist()\n",
    "    l = [0] * 50\n",
    "    features = {\n",
    "        'wordvector': vc[i],\n",
    "        'word.istitle()': int(lemma[i].istitle()),\n",
    "        'word.isdigit()': int(lemma[i].isdigit()),\n",
    "        'wordvector[-1]': l,\n",
    "        '-1:word.istitle()': 0,\n",
    "        'wordvector[-2]': l,\n",
    "        '-2:word.istitle()': 0,\n",
    "        'wordvector[-3]': l,\n",
    "        '-3:word.istitle()': 0,\n",
    "        'wordvector[+1]': l,\n",
    "        '+1:word.istitle()': 0,\n",
    "        'wordvector[+2]': l,\n",
    "        '+2:word.istitle()': 0,\n",
    "        'wordvector[+3]': l,\n",
    "        '+3:word.istitle()': 0\n",
    "    }\n",
    "    if i == 1:\n",
    "        features.update({\n",
    "            'wordvector[-1]': vc[i-1],\n",
    "            '-1:word.istitle()': int(lemma[i-1].istitle()),\n",
    "        })\n",
    "\n",
    "    if i == 2:\n",
    "        features.update({\n",
    "            'wordvector[-2]': vc[i-2],\n",
    "            '-2:word.istitle()': int(lemma[i-2].istitle())\n",
    "        })\n",
    "    if i >= 3:\n",
    "        features.update({\n",
    "            'wordvector[-1]': vc[i-1],\n",
    "            '-1:word.istitle()': int(lemma[i-1].istitle()),\n",
    "            'wordvector[-2]': vc[i-2],\n",
    "            '-2:word.istitle()': int(lemma[i-2].istitle()),\n",
    "            'wordvector[-3]': vc[i-3],\n",
    "            '-3:word.istitle()': int(lemma[i-3].istitle())\n",
    "        })\n",
    "    if i == len(df)-2:\n",
    "        features.update({\n",
    "            'wordvector[+1]': vc[i+1],\n",
    "            '+1:word.istitle()': int(lemma[i+1].istitle()),\n",
    "        })\n",
    "    if i == len(df)-3:\n",
    "        features.update({\n",
    "            'wordvector[+2]': vc[i+2],\n",
    "            '+2:word.istitle()': int(lemma[i+2].istitle())\n",
    "        })\n",
    "    if i <= len(df)-4:\n",
    "        features.update({\n",
    "            'wordvector[+1]': vc[i+1],\n",
    "            '+1:word.istitle()': int(lemma[i+1].istitle()),\n",
    "            'wordvector[+2]': vc[i+2],\n",
    "            '+2:word.istitle()': int(lemma[i+2].istitle()),\n",
    "            'wordvector[+3]': vc[i+3],\n",
    "            '+3:word.istitle()': int(lemma[i+3].istitle())\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block employs the function above on every word to gather its features and puts them in list X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(df)):\n",
    "    p = word2features(i, df, vectorlist).values()\n",
    "    new_p = []\n",
    "    for a in p:\n",
    "        if type(a)!=list:\n",
    "            new_p.append(a)\n",
    "        else:\n",
    "            new_p.extend(a)\n",
    "    X.append(new_p)\n",
    "\n",
    "outfile = open('X','wb')\n",
    "pickle.dump(X, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('X','rb')\n",
    "X = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we haven't missed any word, we take a look at length of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048575\n",
      "1048575\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "y = df['label'].tolist()\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure every word has the same amount of features, we can take a quick look at its parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "365\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))\n",
    "print(len(X[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear SVC model\n",
    "Here, we will introduce X and y into a Support Vector Classifier (SVC) with a linear kernel. I have also tried linear classification *via* Linear Discriminant Analysis (LDA), but this performed worse on all fronts in comparison to SVC in this case. This is likely due to the fact that LDA makes a lot of assumptions on data like for example that the data should be normally distributed whilst SVC does not make any assumptions. \n",
    "\n",
    "I have also tried SVC with a Radial Basis Fucntion kernel (RBF), but fitting such a model took a large amount of time. Fitting a SVC with RBF takes more than double the time of a linear SVC so I decided to focus on linear SVC from here. SVC with RBF is used and discussed later in the very last part of this notebook. This all applies too MLPC and XGBoost too.\n",
    "\n",
    "For the C value, I have tried 0.1, 1 and 10 and 10 seems to give the best results without overfitting when using it on the test set later. A more refined way to find the optimal C-value, would be a grid search. This would have taken up too much time computationally, but it would be a nice implementation to further refine the model.\n",
    "\n",
    "The `class_weight` parameter is set to 'balanced'. This means that the function will automatically adjust the weight of the classes. This yields far better results than leaving it on defaut ('None') because our dataset contains much more non-location words than locations. The maximum iterations has been set to a higher value because it always exceeds the default value (1000). It seems to perform better with more iterations, but that also comes at the cost of computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfatj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight='balanced', max_iter=2000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "svm1 = LinearSVC(C = 1, class_weight='balanced',  max_iter=2000)\n",
    "svm1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfatj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight='balanced', max_iter=2000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm2 = LinearSVC(C = 0.1, class_weight='balanced',  max_iter=2000)\n",
    "svm2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfatj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10, class_weight='balanced', max_iter=2000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm3 = LinearSVC(C = 10, class_weight='balanced',  max_iter=2000)\n",
    "svm3.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests provide first the accuracy and then the F1-score. Here we can see the type of class imbalance in data/prediction. A high accuracy doesn't necessarily mean that we have a good model. This could be a result of having a small number of locations and a large number of non-locations in both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8941754285578046\n",
      "0.4338087098502436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred_test = svm1.predict(x_test)\n",
    "print(svm1.score(x_test,y_test))\n",
    "print(f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8719881744271988\n",
      "0.39303640063305445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred_test2 = svm2.predict(x_test)\n",
    "print(svm2.score(x_test,y_test))\n",
    "print(f1_score(y_test, y_pred_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9335193000023841\n",
      "0.5196720181905877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred_test3 = svm3.predict(x_test)\n",
    "print(svm3.score(x_test,y_test))\n",
    "print(f1_score(y_test, y_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will finally use here is a SVC with a linear kernel with the C-value set to 10 and at 5000 iterations fitted on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfatj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10, class_weight='balanced', max_iter=5000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = LinearSVC(C = 10, class_weight='balanced', max_iter = 5000)\n",
    "svm.fit(X,y)\n",
    "\n",
    "outfile = open('svm','wb')\n",
    "pickle.dump(svm, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('svm','rb')\n",
    "svm = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear SVC model with POS\n",
    "In the next block of code, the POS labels of the main word and the 6 words around it are added to the feature set. Then a linear SVC model with the same settings as above will be trained on this feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = X\n",
    "for i in range(len(df)):\n",
    "    X_pos[i].append(df['labelPOS'][i])\n",
    "    if i < (len(df) - 2):\n",
    "        X_pos[i].append(df['labelPOS'][i+1])\n",
    "    else:\n",
    "        X_pos[i].append(0)\n",
    "    if i < (len(df) - 3):\n",
    "        X_pos[i].append(df['labelPOS'][i+2])\n",
    "    else:\n",
    "        X_pos[i].append(0)\n",
    "    if i < (len(df) - 4):\n",
    "        X_pos[i].append(df['labelPOS'][i+3])\n",
    "    else:\n",
    "        X_pos[i].append(0)\n",
    "    if i > 0:\n",
    "        X_pos[i].append(df['labelPOS'][i-1])\n",
    "    else:\n",
    "        X_pos[i].append(0)\n",
    "    if i > 1:\n",
    "        X_pos[i].append(df['labelPOS'][i-2])\n",
    "    else:\n",
    "        X_pos[i].append(0)\n",
    "    if i > 2:\n",
    "        X_pos[i].append(df['labelPOS'][i-3])\n",
    "    else:\n",
    "        X_pos[i].append(0)\n",
    "\n",
    "outfile = open('X_pos','wb')\n",
    "pickle.dump(X_pos, outfile)\n",
    "outfile.close()\n",
    "\n",
    "svm_pos = LinearSVC(C = 10, class_weight='balanced', max_iter = 5000)\n",
    "svm.fit(X_pos,y)\n",
    "\n",
    "outfile = open('svm_pos','wb')\n",
    "pickle.dump(svm_pos, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('X_pos','rb')\n",
    "X_pos = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('svm_pos','rb')\n",
    "svm_pos = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the test data\n",
    "After building the model, we can now finally prepare the (competition) test dataset for prediction. Lemmatiziation and the same features will be applied here too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_dataset.csv')\n",
    "lemma = []\n",
    "for i in df_test['Word']:\n",
    "    lemma.append(lem.lemmatize(i))\n",
    "df_test['lemma'] = lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tags were not provided with the test dataset so we can predict them via the *nltk* package. If the package doesn't do this properly, this could have an impact on the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.pos_tag(lemma)\n",
    "POS_tags = [i[1] for i in text]\n",
    "df_test['POS'] = POS_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vckag = []\n",
    "l = [0] * 50\n",
    "for i in range(len(df_test)):\n",
    "    try:\n",
    "        vckag.append(w2v[df_test['lemma'][i]].tolist())\n",
    "    except KeyError:\n",
    "        vckag.append(l)\n",
    "\n",
    "outfile = open('vckag','wb')\n",
    "pickle.dump(vckag, outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open('dfkag','wb')\n",
    "pickle.dump(df_test, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make feature set of test data (without POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = []\n",
    "for i in range(len(df_test)):\n",
    "    p = word2features(i, df_test, vckag).values()\n",
    "    new_p = []\n",
    "    for a in p:\n",
    "        if type(a)!=list:\n",
    "            new_p.append(a)\n",
    "        else:\n",
    "            new_p.extend(a)\n",
    "    X_pred.append(new_p)\n",
    "\n",
    "outfile = open('X_pred','wb')\n",
    "pickle.dump(X_pred, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('X_pred','rb')\n",
    "X_pred = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make feature set of test data (with POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = [',' if df_test['POS'][i] == '(' or df_test['POS'][i] == ')' or df_test['POS'][i] == \"''\" or df_test['POS'][i] == '#' or df_test['POS'][i] == 'SYM' else df_test['POS'][i] for i in range(len(df_test))]\n",
    "POS_test = le.transform(new)\n",
    "df_test['labelPOS'] = POS_test\n",
    "\n",
    "X_pred_pos = X_pred\n",
    "for i in range(len(df_test)):\n",
    "    X_pred_pos[i].append(df_test['labelPOS'][i])\n",
    "    if i < (len(df_test) - 2):\n",
    "        X_pred_pos[i].append(df_test['labelPOS'][i+1])\n",
    "    else:\n",
    "        X_pred_pos[i].append(0)\n",
    "    if i < (len(df_test) - 3):\n",
    "        X_pred_pos[i].append(df_test['labelPOS'][i+2])\n",
    "    else:\n",
    "        X_pred_pos[i].append(0)\n",
    "    if i < (len(df_test) - 4):\n",
    "        X_pred_pos[i].append(df_test['labelPOS'][i+3])\n",
    "    else:\n",
    "        X_pred_pos[i].append(0)\n",
    "    if i > 0:\n",
    "        X_pred_pos[i].append(df_test['labelPOS'][i-1])\n",
    "    else:\n",
    "        X_pred_pos[i].append(0)\n",
    "    if i > 1:\n",
    "        X_pred_pos[i].append(df_test['labelPOS'][i-2])\n",
    "    else:\n",
    "        X_pred_pos[i].append(0)\n",
    "    if i > 2:\n",
    "        X_pred_pos[i].append(df_test['labelPOS'][i-3])\n",
    "    else:\n",
    "        X_pred_pos[i].append(0)\n",
    "\n",
    "outfile = open('X_pred_pos','wb')\n",
    "pickle.dump(X_pred_pos, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pos = svm_pos.predict(X_pred_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idlist = [x for x in range(len(df_test))]\n",
    "\n",
    "data_pred = {'Id':idlist,\n",
    "        'Predicted':y_pred}\n",
    "df_pred = pd.DataFrame(data_pred)\n",
    "df_pred.to_csv('predictions.csv',index=False)\n",
    "\n",
    "data_pred = {'Id':idlist,\n",
    "        'Predicted':y_pred_pos}\n",
    "df_pred = pd.DataFrame(data_pred)\n",
    "df_pred.to_csv('predictions_pos.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model without POS features yielded an F1-score of almost 0.38 in the Kaggle leaderboard which is not too far off the F1-score in the testing phase but it can use some more refinement to reduce overfitting. In conclusion, this model could have potential for some better predictions after some more finetuning and an addition of more interesting features. One of the main drawbacks of this model is that it is very computationally heavy and could use some more refinement in terms of feature engineering and its code.\n",
    "\n",
    "The same could be said for the model with POS features. This yielded an F1-score of almost 0.37 in the Kaggle leaderboard. From this, it seems that adding POS features makes no significant difference to the model. What could have happened here is that because the feature vectors are so large (365 units long), adding more features would not influence the model at all. An intersting thing to try out later could be using even smaller word embeddings or making them smaller by ourselves by using a dimensional reduction method such as Principle Component Analysis (PCA) to see if the POS tags would make a difference. \n",
    "\n",
    "Also, because no normilization has been done, it could be that some features have more importance than others. Because of computational limits, I couldn't normalize the full feature sets so I only took the first 100000 words to train the model. The code for is not included, but it was easily done in just 2 lines with *sklearn*s StandardScaler function. In hindsight, I could have done this in batches, but this was not possible due to time constraints. The model with normalized smaller feature set without POS scored around 0.36 on Kaggle and the one with POS score scored 0.38. This is very similar to the scores above so it could be that both normalization and splicing the dataset doesn't affect the model that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models\n",
    "\n",
    "Here is the section were I experimented with other models.\n",
    "\n",
    "Here I tried a MLPClassifier and this model actually yielded my highest score of 0.41. This is definitely a model to look further into later, but due to time constraints, this was not possible now. There are many parameters here to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=500)\n",
    "mlp.fit(X, y)\n",
    "y_pred_mlp = mlp.predict(X_pred)\n",
    "\n",
    "outfile = open('mlp','wb')\n",
    "pickle.dump(mlp, outfile)\n",
    "outfile.close()\n",
    "\n",
    "idlist = [x for x in range(len(df_test))]\n",
    "\n",
    "data_pred = {'Id':idlist,\n",
    "        'Predicted':y_pred_mlp}\n",
    "df_pred = pd.DataFrame(data_pred)\n",
    "df_pred.to_csv('predictions_mlp.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tried an SVC with a RBF kernel, but this did not yield good results at all at a much higher computational time so I stcuk with linear SVC. This model could use much more refinement like optimizing the C and gamma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfatj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svmrbf = SVC(C = 10, class_weight='balanced',  max_iter=5000)\n",
    "svmrbf.fit(X,y)\n",
    "\n",
    "outfile = open('svmrbf','wb')\n",
    "pickle.dump(svmrbf, outfile)\n",
    "outfile.close()\n",
    "\n",
    "y_pred_rbf = svmrbf.predict(X_pred)\n",
    "\n",
    "idlist = [x for x in range(len(df_test))]\n",
    "\n",
    "data_pred = {'Id':idlist,\n",
    "        'Predicted':y_pred_rbf}\n",
    "df_pred = pd.DataFrame(data_pred)\n",
    "df_pred.to_csv('predictions_rbf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9678516081348497\n",
      "0.6350546714301181\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500)\n",
    "mlp.fit(x_train, y_train)\n",
    "y_pred_mlp2 = mlp.predict(x_test)\n",
    "\n",
    "print(mlp.score(x_test,y_test))\n",
    "print(f1_score(y_test, y_pred_mlp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfatj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6130415087142074\n",
      "0.6350546714301181\n"
     ]
    }
   ],
   "source": [
    "svmrbf = SVC(C = 10, class_weight='balanced',  max_iter=5000)\n",
    "svmrbf.fit(x_train,y_train)\n",
    "y_pred_rbf = svmrbf.predict(X_pred)\n",
    "\n",
    "print(svmrbf.score(x_test,y_test))\n",
    "print(f1_score(y_test, y_pred_mlp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tried an XGBoost model and this also yielded a score of approximately 0.39. I have not looked much into this model but it surely has potential with more refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = df['label'].value_counts().tolist()[0] / df['label'].value_counts().tolist()[1]\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(scale_pos_weight = weight)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_pred)\n",
    "\n",
    "outfile = open('xgb','wb')\n",
    "pickle.dump(xgb_model, outfile)\n",
    "outfile.close()\n",
    "\n",
    "idlist = [x for x in range(len(df_test))]\n",
    "\n",
    "data_pred = {'Id':idlist,\n",
    "        'Predicted':y_pred_xgb}\n",
    "df_pred = pd.DataFrame(data_pred)\n",
    "df_pred.to_csv('predictions_xgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Because all of the models (except for the SVC with RBF kernel) yielded very similar results, it is very possible that our traning dataset or most likely our builded feature set could use much more refinement. Perhaps more relevant features could be implemented and maybe even a dimensional reduction of the word embeddings could be of use in this case, both in terms of better predictions and computational efficiency."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76fdcbccaa7318cdd40630ed1d6c7985538af87fb0f5dc74019909bab7211608"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
